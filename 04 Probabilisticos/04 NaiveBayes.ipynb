{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4094e1c5",
   "metadata": {},
   "source": [
    "# Algoritmo Naïve Bayes\n",
    "\n",
    "El algoritmo de Bayes naïve (o ingenuo) es un modelo gráfico dirigido, que entra dentro de la familia de las redes bayesianas. De hecho, puede pensarse como una red bayesiana bastante simple donde se tiene un vector de evidencias $x = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_d \\end{pmatrix}$ y una variable $Y$ sobre la que queremos obtener una consulta, su probabilidad.\n",
    "\n",
    "El algoritmo de Bayes naïve puede considerarse como un algoritmo de clasificación, en tanto con la estimación de la probabilidad realizada, puede eligir el valor $y_i$ de la variable $Y$ que maximiza la probabilidad de la evidencia. Este valor de probabilidad máxima se dice que es la clase del vector $x$.\n",
    "\n",
    "Presentamos una implementación del algoritmo de Bayes naïve para un problema particular que la clasificación de texto en lenguas a partir de la frecuencia de los conjuntos de caracteres que en ellos se presentan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a07123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, cess_esp\n",
    "from nltk import ngrams\n",
    "from elotl.corpus import load\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter, defaultdict\n",
    "from re import sub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992fb76",
   "metadata": {},
   "source": [
    "### Preparación de los datos\n",
    "\n",
    "En primer lugar, obtenemos los datos con los que vamos a trabajar. Para esto utilizamos dos paqueterías que cuentan con datos textuales en diferentes lenguas:\n",
    "\n",
    "1. NLTK (Natural language toolkit): Es una paquetería que cuenta con diferentes conjuntos de datos textuales. En particular usamos dos:\n",
    "    1. Brown corpus: es un conjunto de datos con textos en inglés.\n",
    "    2. Cess corpus: es un conjunto de datos con textos en español.\n",
    "2. Elotl: Es una paquetería que cuenta con datos textuales para conjuntos de datos en lenguas indígenas habladas en México; en particular tomamos dos:\n",
    "    1. Tsunkua corpus: conjunto de datos textuales para el otomí.\n",
    "    1. Axolotl corpus: conjunto de datos textuales para el náhuatl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743411f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos a usar en inglés, español, otomí y náhua\n",
    "eng = brown.sents()\n",
    "esp = cess_esp.sents()\n",
    "oto = [sent[1].split() for sent in load('tsunkua')]\n",
    "nah = [sent[1].split() for sent in load('axolotl')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab2056",
   "metadata": {},
   "source": [
    "Para hacer la clasificaicón usaremos conjuntos de caracteres (n-gramas); es decir, un texto se clasificará en una lengua según los patrones de caracteres que contenga. En este caso, podemos pensar un n-grama como la creación de un conjunto de subcadenas de tamaño n, a partir de la cadena completa.\n",
    "\n",
    "Por ejemplo, si pensamos en la palabra \"gato\" y usamos $n=2$ (2-gramas o bigramas), tenemos que la función de <tt>get_ngrams</tt> realizará lo siguiente:\n",
    "\n",
    "$$gato \\mapsto \\{ga, at, to\\}$$\n",
    "\n",
    "Es decir, obtiene tres elementos que corresponde a subcadenas de longitud $n=2$. Estas subcadenas se obtienen al tomar una ventana de tamaño $n$ que recorre la cadena.\n",
    "\n",
    "Hacemos esto puesto que nuestro objetivo es detectar de que idioma se trata. Obtener estos patrones de caracteres nos permite detectar con alto grado de precisión la lengua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906675a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(word,n):\n",
    "    \"\"\"Función para obtener n-gramas.\"\"\"\n",
    "    #Limpia el texto\n",
    "    clean_word = sub('[^\\w\\s)]','',word.lower())        \n",
    "    if len(clean_word) <= n and clean_word != '':\n",
    "        #Si no se peuden obtener n-gramas\n",
    "        ngram_list = [clean_word]\n",
    "    else:\n",
    "        #Obtiene n-gramas\n",
    "        ngram_list = [''.join(ngram) for ngram in  ngrams(clean_word,n)]\n",
    "    \n",
    "    return ngram_list\n",
    "\n",
    "def process(sent, n=2):\n",
    "    \"\"\"Función para procesar las oraciones de un texto.\"\"\"\n",
    "    sent_ngrams = list(chain(*[get_ngrams(w,n) for w in sent]))\n",
    "    \n",
    "    return sent_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9767ca8",
   "metadata": {},
   "source": [
    "Podemos observar qué tipo de procesamiento es el que se hace para una cadena con varias palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8a9ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y', 'el', 'qu', 'ue', 'de', 'el', 'ni', 'iñ', 'ño', 'de', 'aq', 'qu', 'uí']\n"
     ]
    }
   ],
   "source": [
    "input_text = 'y el ¿que del niño de aquí?.'.split()\n",
    "print(process(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc79f65",
   "metadata": {},
   "source": [
    "#### Generación del conjunto de datos\n",
    "\n",
    "Definiremos una clase que nos permitirá manejar el conjunto de datos de manera simple. Lo que hace esta clase es:\n",
    "\n",
    "1. Guarda las clases (lenguas) que consideramos para clasificar.\n",
    "2. Genera los n-gramas de los textos de los conjuntos de datos que hemos exportado anteriormente.\n",
    "3. Asocia cada uno de los n-gramas a las lenguas (clases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f072ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"Clase para crear el dataset de las lenguas.\"\"\"\n",
    "    def __init__(self):\n",
    "        #Lenguas a considerar\n",
    "        self.languages = {'english': eng,'spanish':esp,'nahuatl':nah,'otomi':oto}\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        \n",
    "    def get_dataset(self):\n",
    "        \"\"\"Función para crear el dataset (pares [x,y]) a partir de los textos.\"\"\"\n",
    "        for lang, sentences in self.languages.items():\n",
    "            print(lang)\n",
    "            for sent in sentences:\n",
    "                #Procesa los textos\n",
    "                x = process(sent)\n",
    "                #Genera los inputs y las clases\n",
    "                self.X.append(x)\n",
    "                self.Y.append(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4990e916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n",
      "spanish\n",
      "nahuatl\n",
      "otomi\n"
     ]
    }
   ],
   "source": [
    "#Creamos el dataset\n",
    "dataset = Dataset()\n",
    "dataset.get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e4d9e",
   "metadata": {},
   "source": [
    "Podemos observar de que tamaños son los datos que tenemos; es decir, cuántas evidencias podemos usar para etimar las probabilidades del modelo de bayes naïve. Aquí $X$ corresponde a las evidencias y $Y$ a las clases que se asocian con cada una de las evidencias. El conjunto de datos es un conjunto de pares:\n",
    "\n",
    "$$\\mathcal{S} = \\{(x,y) : x \\text{ es evidencia}, y \\text{ es clase}\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d756c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84450 84450\n"
     ]
    }
   ],
   "source": [
    "#Imprime longitud del dataset\n",
    "print(len(dataset.X), len(dataset.Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844011a",
   "metadata": {},
   "source": [
    "Finalmente, separaremos el conjunto de datos en dos partes. Esta partición la hacemos de forma aleatoria, y conformamos los dos conjuntos:\n",
    "\n",
    "1. Conjunto de entrenamiento: Refiere a los datos que nos van a servir para estimar las probabilidades del modelo de bayes ingenuo. Corresponde a un 70% de los datos totales.\n",
    "2. Conjunto de evaluación: Estos datos nos servirán para poder evaluar qué también trabaja nuestro modelo. Corresponde al 30% de los datos totales. Si nuestra evaluación es satisfactoria, podemos integrar esta parte de los datos a los de entrenamiento para hacer una estimación más completa del modelo.\n",
    "\n",
    "Para hacer esta separación, utilizamos la función <tt>train_test_split</tt> de la paquetería <tt>sklearn</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1e2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separación de los datos\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset.X, dataset.Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a1093",
   "metadata": {},
   "source": [
    "## Modelo de Bayes Naïve\n",
    "\n",
    "Ahora definimos el modelo de Naïve Bayes con el cual podremos realizar la clasificación que esperamos. Como señalamos, el modelo de Bayes naïve es una red bayesiana con la siguiente estructura:\n",
    "\n",
    "![image](images/BayesNaive.png)\n",
    "\n",
    "Como se ve en la gráfica, se tiene un conjunto de variables $X_1, X_2,...,X_d$ que son independientes entre sí; es decir, no tienen ninguna arista que conecte alguna variable $X_i$ con otra $X_j$. De hecho, el nombre de naïve o ingenuo del método proviene precisamente en asumir que las observaciones o evidencia son independientes entre sí.\n",
    "\n",
    "Al mismo tiempo, podemos observar que cada $X_i$ tiene un úinico nodo padre que es la variable $Y$; es decir, $Y$ genera la evidencia (de allí también que se le conozca como un modelo generativo). De esta forma tenemos que para toda $i \\in \\{1,2,...,d\\}$ se tiene que:\n",
    "\n",
    "$$p\\big(X_i | \\pi(X_i) \\big) = p(X_i | Y)$$\n",
    "\n",
    "El caso de la variable $Y$, como no cuenta con padres, sabemos que sólo debemos estimar la probabilidad $p(Y)$ para cada valor posible de $Y$. Nuestro objetivo es estimar la probabilidad conjunta $p(Y=y,X_1=x_1,...,X_d=x_d)$ dada una evidencia $x = \\begin{pmatrix} x_1 & \\cdots & x_d \\end{pmatrix}$. Por lo que sabemos de las redes bayesianas, esta probabilidad conjunta se estima como:\n",
    "\n",
    "$$p(Y=y,X_1=x_1,...,X_d=x_d) = p(Y=y)\\prod_{i=1}^d p(x_i|y)$$\n",
    "\n",
    "También podemos calcular la probabilidad condicional; esto es, estimar:\n",
    "\n",
    "$$p(Y=y|X_1=x_1,...,X_d=x_d) = \\frac{p(Y=y)\\prod_{i=1}^d p(x_i|y)}{\\sum_y p(Y=y)\\prod_{i=1}^d p(x_i|y)}$$\n",
    "\n",
    "Sin embargo, como lo que queremos encontrar, en general, es la clase $y$ a la que pertenece el vector $x$ esto no es necesario, pues podemos ver que la clase con mayor probabilidad $\\hat{y}$ se obtienen como:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y p(Y=y|X_1=x_1,...,X_d=x_d) = \\arg\\max_y p(Y=y,X_1=x_1,...,X_d=x_d)$$\n",
    "\n",
    "Por lo que, en general, sólo estimaremos la probabilidad conjunta. A continuación construimos una clase para el modelo de clasificación de Bayes Naïve. En este caso, las tablas de probabilidad condicional que responden a las probabilidades $p(Y)$ y $p(X|Y)$ se estiman de manera frecuentista. En concreto:\n",
    "\n",
    "1. $p(Y=y) = \\frac{frec(y)}{\\sum_y frec(y)}$\n",
    "2. $p(X_i=x_i|Y=y) = \\frac{frec(x_i, y)}{frec(y)}$\n",
    "\n",
    "Donde $frec(\\cdot)$ es la frecuencia de ese ítem. Las funciones que definimos son las siguientes:\n",
    "\n",
    "1. <tt>fit</tt>: Estima las probabilidades del modelo; es decir, obtiene las tablas de probabilidad para cada variable. En este caso $p(X|Y)$ y $p(Y)$.\n",
    "2. <tt>predict_proba</tt>: Obtiene la probabilidad conjunta $p(Y=y, X_1=x_1, ...,X_d=x_d)$ para cada una de las clases $y$ dado un vector de evidencia $x$.\n",
    "3. <tt>predict_logproba</tt>: Sustituye las probabilidades por el logaritmo de las probabilidades para cambiar la estimación de productos por sumas (evitando así el desvanecimiento de probabilidades muy pequeñas): $$\\log p(Y=y,X_1=x_1,...,X_d=x_d) = \\log p(Y=y)+ \\sum_{i=1}^d \\log p(x_i|y)$$\n",
    "4. <tt>predict</tt>: en base al logaritmo de la probabilidad o a la probabilidad, regresa la clase que maximiza el valor. La clase más probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d7c1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(object):\n",
    "    \"\"\"Clase del modelo de bayes ingenuo.\"\"\"\n",
    "    def __init__(self, priors={}):\n",
    "        #Prior, p(Y)\n",
    "        self.prec_priors = priors\n",
    "        self.priors = {}\n",
    "        #Clases a considerar\n",
    "        categories = []\n",
    "        #Condicionales, p(X|Y)\n",
    "        self.conditional = {}\n",
    "    \n",
    "    def count_cat(self, y):\n",
    "        \"\"\"Función para contar las clases.\"\"\"\n",
    "        freqs = Counter(y)\n",
    "        total_freq = sum(freqs.values())\n",
    "        for lang, freq in freqs.items():\n",
    "            self.priors[lang] = freq/total_freq\n",
    "            \n",
    "    def count_cond(self,x,y):\n",
    "        \"\"\"Función para contar las probabilidades condicionales p(x|y)\"\"\"\n",
    "        freq_cat = Counter(y)\n",
    "        print(freq_cat)\n",
    "        joint_elements = defaultdict(list)\n",
    "        for category,example in zip(y,x):\n",
    "            joint_elements[category].append(example)\n",
    "        \n",
    "        for category,examples in joint_elements.items():\n",
    "            freqs = Counter(chain(*examples))\n",
    "            total_freq = sum(freqs.values())\n",
    "            self.conditional[category] = {w:freq/total_freq for w,freq in freqs.items()}\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        \"\"\"Función para entrenar el modelo. Estimar las probabilidades.\"\"\"\n",
    "        if self.prec_priors == {}:\n",
    "            self.count_cat(y)\n",
    "        else:\n",
    "            for i,category in enumerate(set(y)):\n",
    "                self.priors[category] = self.prec_priors[i]\n",
    "        \n",
    "        self.categories = list(self.priors.keys())\n",
    "        self.count_cond(x,y)\n",
    "        \n",
    "    def predict_proba(self,x):\n",
    "        \"\"\"Función para obtener probabilidades de clases. Para cada clase y, obtiene p(y,x1,...,xd)\"\"\"\n",
    "        prediction = np.zeros(len(self.priors))\n",
    "        for i,category in enumerate(self.categories):\n",
    "            p = 1\n",
    "            prior = self.priors[category]\n",
    "            for x_i in x:\n",
    "                try:\n",
    "                    cond = self.conditional[category][x_i]\n",
    "                except:\n",
    "                    cond = 1/prior\n",
    "                p *= cond*prior\n",
    "                \n",
    "            prediction[i] = p\n",
    "            \n",
    "        return prediction\n",
    "    \n",
    "    def predict_logproba(self,x):\n",
    "        \"\"\"Función para obtener logaritmos de probabilidades de clases.\"\"\"\n",
    "        prediction = np.zeros(len(self.priors))\n",
    "        for i,category in enumerate(self.categories):\n",
    "            p = 0\n",
    "            prior = self.priors[category]\n",
    "            for x_i in x:\n",
    "                try:\n",
    "                    cond = self.conditional[category][x_i]\n",
    "                except:\n",
    "                    cond = 1/prior\n",
    "                \n",
    "                p += np.log(cond*prior)\n",
    "                \n",
    "            prediction[i] = p\n",
    "            \n",
    "        return prediction\n",
    "        \n",
    "    def predict(self,x,log=True):\n",
    "        \"\"\"Función para predecir las clases de un ejemplo de evaluación.\"\"\"\n",
    "        if log:\n",
    "            probas = self.predict_logproba(x)\n",
    "        else:\n",
    "            probas = self.predict_proba(x)\n",
    "        y_hat = np.argmax(probas)\n",
    "        \n",
    "        #print(self.categories)\n",
    "        #print(probas)\n",
    "        \n",
    "        return self.categories[y_hat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0b1df",
   "metadata": {},
   "source": [
    "### Aplicación del modelo a los datos\n",
    "\n",
    "Construimos el modelo en base a unos prios uniformes, pues asumimos que la distribuicón de las lenguas es la misma en cada caso (cada lengua tiene la misma probabilidad de suceder independientemente de las evidencias) y lo entrenamos con nuestros datos de entrenamiento que hemos obtenido anteriormente. Se imprime el número de datos observados en cada lengua para hacer la estimación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f696837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'english': 40056, 'nahuatl': 11346, 'spanish': 4243, 'otomi': 3470})\n"
     ]
    }
   ],
   "source": [
    "#Modelo\n",
    "clf = NaiveBayesClassifier(priors=[0.25,0.25,0.25,0.25])\n",
    "#Entrenamiento\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad4f20",
   "metadata": {},
   "source": [
    "Podemos ver cómo se comporta el modelo. Al tener activada la opción log (log=True) los valores serán logarítmicos, por lo que el método regresa valores negativos. La función imprimirá las probabilidades logarítmicas para cada lengua y regresará la clase con mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f5a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lengua más probable es: english\n"
     ]
    }
   ],
   "source": [
    "#Texto de entrada\n",
    "input_text = 'hello'\n",
    "\n",
    "#Imprime la clase\n",
    "print('La lengua más probable es: {}'.format(clf.predict(process(input_text.split()), log=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d35449e",
   "metadata": {},
   "source": [
    "Por ejemplo, si tomamos un texto en español obtenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c21af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lengua más probable es: spanish\n"
     ]
    }
   ],
   "source": [
    "#Texto de entrada\n",
    "input_text = 'hola' #'ra detha' #'tinechmacasnequi'\n",
    "\n",
    "#Imprime la clase\n",
    "print('La lengua más probable es: {}'.format(clf.predict(process(input_text.split()), log=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a5bc1",
   "metadata": {},
   "source": [
    "Si tomamos otra lengua, también podemos ver los valores que arrojan. En este caso, usamos log=False, por lo que regresa probabilidades no logarítmicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd462d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lengua más probable es: otomi\n"
     ]
    }
   ],
   "source": [
    "#Texto de entrada\n",
    "input_text = 'ra detha' #'tinechmacasnequi'\n",
    "\n",
    "#Imprime la clase\n",
    "print('La lengua más probable es: {}'.format(clf.predict(process(input_text.split()), log=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aaf4bf",
   "metadata": {},
   "source": [
    "### Evaluación del modelo\n",
    "\n",
    "Para evaluar el modelo, usamos el dataset de evaluación y predecimos las clases a las que pertenece. Calculamos métricas de evaluación para saber qué tan bien trabaja nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eaceb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     english       0.89      0.94      0.92     17284\n",
      "     nahuatl       0.97      0.86      0.91      4771\n",
      "       otomi       0.43      0.53      0.47      1493\n",
      "     spanish       0.61      0.34      0.44      1787\n",
      "\n",
      "    accuracy                           0.86     25335\n",
      "   macro avg       0.72      0.67      0.69     25335\n",
      "weighted avg       0.86      0.86      0.86     25335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [clf.predict(x) for x in x_test]\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e1ce5",
   "metadata": {},
   "source": [
    "En este caso, podemos ver que el accuracy es de 0.86; es decir, que acierta en el 86\\% de los casos.\n",
    "Podemos ver cómo trabaja en casos individuales de clasificación. Finalmente, podemos explorar cuáles son los rasgos que más influyen para la decisión en cada una de las clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd7dec23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('th', 0.03553896271876417),\n",
       " ('he', 0.03264527433037059),\n",
       " ('in', 0.023555532493582942),\n",
       " ('er', 0.02020582305550011),\n",
       " ('an', 0.019216893686115864),\n",
       " ('re', 0.017798834873793145),\n",
       " ('on', 0.01628419353018242),\n",
       " ('at', 0.014110897198689963),\n",
       " ('en', 0.013809408199061521),\n",
       " ('nd', 0.012940165417469469),\n",
       " ('ed', 0.012443617580141723),\n",
       " ('or', 0.012217122075395733),\n",
       " ('es', 0.012211819505050509),\n",
       " ('ti', 0.011399011222132625),\n",
       " ('te', 0.011339167928236528),\n",
       " ('it', 0.011027452542942297),\n",
       " ('is', 0.010989198285451755),\n",
       " ('st', 0.01094791398776394),\n",
       " ('to', 0.010846407641155369),\n",
       " ('ar', 0.0104752277169897),\n",
       " ('of', 0.010452881170534828),\n",
       " ('ng', 0.010061248475037582),\n",
       " ('ha', 0.00989156622399042),\n",
       " ('nt', 0.009887021163694512),\n",
       " ('al', 0.009883233613447924),\n",
       " ('ou', 0.009616590076088097),\n",
       " ('as', 0.009342371438235093),\n",
       " ('hi', 0.008543955846254246),\n",
       " ('se', 0.008455705925508735),\n",
       " ('le', 0.008439040704423745),\n",
       " ('ve', 0.008079980941047159),\n",
       " ('me', 0.007866741862164229),\n",
       " ('co', 0.007481169247061524),\n",
       " ('de', 0.00734140864296241),\n",
       " ('ne', 0.007330045992222645),\n",
       " ('ea', 0.007245583621723722),\n",
       " ('ro', 0.007207708119257837),\n",
       " ('io', 0.006761534700209717),\n",
       " ('ri', 0.00664222686744218),\n",
       " ('ic', 0.006386567225797459),\n",
       " ('ll', 0.006321800116580797),\n",
       " ('ra', 0.006213097424503708),\n",
       " ('li', 0.00616954059666794),\n",
       " ('a', 0.0061430277449418215),\n",
       " ('ce', 0.006012357261434519),\n",
       " ('be', 0.0059036545693574305),\n",
       " ('ch', 0.005698748101016995),\n",
       " ('om', 0.005434755848829779),\n",
       " ('ma', 0.005424529463163991),\n",
       " ('el', 0.005304085365322478)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "sorted(clf.conditional['english'].items(),key=itemgetter(1),reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877e282",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
